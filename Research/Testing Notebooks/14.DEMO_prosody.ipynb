{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a361cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n",
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: en (0.96) in first 30s of audio...\n",
      "Language detected: en\n",
      "Transcription time: 17.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Temp\\ipykernel_20972\\2837444162.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio sr=16000, duration=3.22s\n",
      "\n",
      "--- Word-level transcription ---\n",
      "[0.64 - 0.82] I\n",
      "[0.90 - 1.16] woke\n",
      "[1.25 - 1.33] up\n",
      "[1.47 - 1.73] very\n",
      "[1.79 - 2.12] happy\n",
      "[2.18 - 2.68] today.\n",
      "\n",
      "--- Full Transcript ---\n",
      "I woke up very happy today.\n",
      "\n",
      "--- Prosody Statistics ---\n",
      "Average pause: 0.09s\n",
      "Total silence: 0.43s\n",
      "Filler count: 0\n",
      "Filler percentage: 0.00%\n",
      "Detected fillers: []\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "audio_file = \"Smooth.m4a\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model = whisperx.load_model(\"small\", device=device, compute_type=\"float32\")\n",
    "\n",
    "start_time = time.time()\n",
    "result = model.transcribe(audio_file)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Language detected: {result['language']}\")\n",
    "print(f\"Transcription time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "align_model, align_metadata = whisperx.load_align_model(\n",
    "    language_code=result[\"language\"], device=device\n",
    ")\n",
    "\n",
    "audio, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
    "duration_sec = len(audio) / sr\n",
    "print(f\"Loaded audio sr={sr}, duration={duration_sec:.2f}s\")\n",
    "\n",
    "result_aligned = whisperx.align(\n",
    "    transcript=result[\"segments\"],\n",
    "    model=align_model,\n",
    "    align_model_metadata=align_metadata,\n",
    "    audio=audio,\n",
    "    device=device,\n",
    "    return_char_alignments=False\n",
    ")\n",
    "\n",
    "word_segments = result_aligned[\"word_segments\"]\n",
    "\n",
    "print(\"\\n--- Word-level transcription ---\")\n",
    "for w in word_segments:\n",
    "    word = w[\"word\"].strip()\n",
    "    start = w[\"start\"]\n",
    "    end = w[\"end\"]\n",
    "    print(f\"[{start:.2f} - {end:.2f}] {word}\")\n",
    "\n",
    "transcript_text = \" \".join(w[\"word\"].strip() for w in word_segments)\n",
    "transcript_text = \" \".join(transcript_text.split())\n",
    "\n",
    "print(\"\\n--- Full Transcript ---\")\n",
    "print(transcript_text)\n",
    "\n",
    "def compute_pause_stats(word_segments):\n",
    "    pauses = []\n",
    "    total_silence = 0.0\n",
    "\n",
    "    for i in range(1, len(word_segments)):\n",
    "        prev_end = word_segments[i - 1][\"end\"]\n",
    "        curr_start = word_segments[i][\"start\"]\n",
    "        pause = curr_start - prev_end\n",
    "        if pause > 0:\n",
    "            pauses.append(pause)\n",
    "            total_silence += pause\n",
    "\n",
    "    avg_pause = np.mean(pauses) if pauses else 0.0\n",
    "    return avg_pause, total_silence\n",
    "\n",
    "def detect_fillers(word_segments, language_code):\n",
    "    filler_words_en = {\n",
    "        \"um\", \"uh\", \"uhm\", \"umm\", \"ah\", \"hmm\", \"like\",\n",
    "        \"you know\", \"i mean\", \"so\", \"okay\", \"alright\",\n",
    "        \"well\", \"basically\", \"actually\", \"literally\"\n",
    "    }\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    words = [\n",
    "        w[\"word\"].strip().lower().translate(translator)\n",
    "        for w in word_segments\n",
    "    ]\n",
    "\n",
    "    filler_count = 0\n",
    "    detected = []\n",
    "\n",
    "    for filler in filler_words_en:\n",
    "        tokens = filler.split()\n",
    "        n = len(tokens)\n",
    "        for i in range(len(words) - n + 1):\n",
    "            if words[i:i + n] == tokens:\n",
    "                filler_count += 1\n",
    "                detected.append(\" \".join(words[i:i + n]))\n",
    "\n",
    "    filler_pct = (filler_count / len(words)) * 100 if words else 0\n",
    "    return filler_count, filler_pct, detected\n",
    "\n",
    "avg_pause, total_silence = compute_pause_stats(word_segments)\n",
    "filler_count, filler_pct, detected_fillers = detect_fillers(\n",
    "    word_segments, result[\"language\"]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Prosody Statistics ---\")\n",
    "print(f\"Average pause: {avg_pause:.2f}s\")\n",
    "print(f\"Total silence: {total_silence:.2f}s\")\n",
    "print(f\"Filler count: {filler_count}\")\n",
    "print(f\"Filler percentage: {filler_pct:.2f}%\")\n",
    "print(f\"Detected fillers: {detected_fillers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efab213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n",
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: en (0.94) in first 30s of audio...\n",
      "Language detected: en\n",
      "Transcription time: 23.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Temp\\ipykernel_20972\\2153512895.py:27: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded audio sr=16000, duration=9.66s\n",
      "\n",
      "--- Word-level transcription ---\n",
      "[0.54 - 2.83] I\n",
      "[2.85 - 3.15] like\n",
      "[4.09 - 4.43] woke\n",
      "[4.57 - 4.65] up\n",
      "[6.70 - 7.04] just\n",
      "[7.79 - 8.05] very\n",
      "[8.09 - 8.41] happy\n",
      "[8.49 - 8.91] today.\n",
      "\n",
      "--- Full Transcript ---\n",
      "I like woke up just very happy today.\n",
      "\n",
      "--- Prosody Statistics ---\n",
      "Average pause: 0.57s\n",
      "Total silence: 4.02s\n",
      "Filler count: 2\n",
      "Filler percentage: 25.00%\n",
      "Detected fillers: ['like', 'just']\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "audio_file = \"Filler.m4a\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model = whisperx.load_model(\"small\", device=device, compute_type=\"float32\")\n",
    "\n",
    "start_time = time.time()\n",
    "result = model.transcribe(audio_file)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Language detected: {result['language']}\")\n",
    "print(f\"Transcription time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "align_model, align_metadata = whisperx.load_align_model(\n",
    "    language_code=result[\"language\"], device=device\n",
    ")\n",
    "\n",
    "audio, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
    "duration_sec = len(audio) / sr\n",
    "print(f\"Loaded audio sr={sr}, duration={duration_sec:.2f}s\")\n",
    "\n",
    "result_aligned = whisperx.align(\n",
    "    transcript=result[\"segments\"],\n",
    "    model=align_model,\n",
    "    align_model_metadata=align_metadata,\n",
    "    audio=audio,\n",
    "    device=device,\n",
    "    return_char_alignments=False\n",
    ")\n",
    "\n",
    "word_segments = result_aligned[\"word_segments\"]\n",
    "\n",
    "print(\"\\n--- Word-level transcription ---\")\n",
    "for w in word_segments:\n",
    "    word = w[\"word\"].strip()\n",
    "    start = w[\"start\"]\n",
    "    end = w[\"end\"]\n",
    "    print(f\"[{start:.2f} - {end:.2f}] {word}\")\n",
    "\n",
    "transcript_text = \" \".join(w[\"word\"].strip() for w in word_segments)\n",
    "transcript_text = \" \".join(transcript_text.split())\n",
    "\n",
    "print(\"\\n--- Full Transcript ---\")\n",
    "print(transcript_text)\n",
    "\n",
    "def compute_pause_stats(word_segments):\n",
    "    pauses = []\n",
    "    total_silence = 0.0\n",
    "\n",
    "    for i in range(1, len(word_segments)):\n",
    "        prev_end = word_segments[i - 1][\"end\"]\n",
    "        curr_start = word_segments[i][\"start\"]\n",
    "        pause = curr_start - prev_end\n",
    "        if pause > 0:\n",
    "            pauses.append(pause)\n",
    "            total_silence += pause\n",
    "\n",
    "    avg_pause = np.mean(pauses) if pauses else 0.0\n",
    "    return avg_pause, total_silence\n",
    "\n",
    "def detect_fillers(word_segments, language_code):\n",
    "    filler_words_en = {\n",
    "        \"um\", \"uh\", \"uhm\", \"umm\", \"ah\", \"hmm\", \"like\",\n",
    "        \"you know\", \"i mean\", \"so\", \"okay\", \"alright\",\n",
    "        \"well\", \"basically\", \"actually\", \"literally\", \"just\"\n",
    "    }\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    words = [\n",
    "        w[\"word\"].strip().lower().translate(translator)\n",
    "        for w in word_segments\n",
    "    ]\n",
    "\n",
    "    filler_count = 0\n",
    "    detected = []\n",
    "\n",
    "    for filler in filler_words_en:\n",
    "        tokens = filler.split()\n",
    "        n = len(tokens)\n",
    "        for i in range(len(words) - n + 1):\n",
    "            if words[i:i + n] == tokens:\n",
    "                filler_count += 1\n",
    "                detected.append(\" \".join(words[i:i + n]))\n",
    "\n",
    "    filler_pct = (filler_count / len(words)) * 100 if words else 0\n",
    "    return filler_count, filler_pct, detected\n",
    "\n",
    "avg_pause, total_silence = compute_pause_stats(word_segments)\n",
    "filler_count, filler_pct, detected_fillers = detect_fillers(\n",
    "    word_segments, result[\"language\"]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Prosody Statistics ---\")\n",
    "print(f\"Average pause: {avg_pause:.2f}s\")\n",
    "print(f\"Total silence: {total_silence:.2f}s\")\n",
    "print(f\"Filler count: {filler_count}\")\n",
    "print(f\"Filler percentage: {filler_pct:.2f}%\")\n",
    "print(f\"Detected fillers: {detected_fillers}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
