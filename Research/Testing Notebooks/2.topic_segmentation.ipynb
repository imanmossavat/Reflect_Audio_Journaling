{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137881d3",
   "metadata": {},
   "source": [
    "# Topic segmentation\n",
    "### This notebook will serve as support to the reserach that I do regarding this topic - exploring how can AI detect topic segmentation.\n",
    "This notebook does not use a dataset but only one journaling etnry for initial exploration. Additional reasearch and experimentation will follow.\n",
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e08935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lenovo X1 Carbon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.4)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralCoclustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95a918",
   "metadata": {},
   "source": [
    "## Load test data\n",
    "After researching available benchmarks, the conclusion turned out to be that they do not cover this personal journaling domain and the data they use is not suitable to identify the best method of topic segmentation and it would be irrelevant to do so. In this case, I decided to create my own dataset with similar jourbaling entries to what may be the case in real-world scenario. I also provided the segmented sentences and topics in the needed format. Here I will load my data first and convert them to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698e61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Today I had a meeting with my semester coach and we started by discussing my individual project.', ' I had some troubles in the beginning but now everything is clear.', ' I enjoy the topic I chose and I am very happy with the progress I make.', ' I look formard to finish it and see the end product.', ' Then I talked with one of my team mates regarding our group work together because I was not satisfied with his way of working.', ' He always misses deadlines and skips our group meetings and I suggested that he tries to put more effort.', 'Finally, I got home and saw my mother making my favourite meal.', ' I have always loved her cooking and appreaciate that she does it for me.', ' Then we sat together on the table, ate the dinner and talked about our days. ']\n",
      "Gold segments: [0, 0, 0, 0, 1, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"topics.csv\")\n",
    "row = df.iloc[0]\n",
    "\n",
    "sentences = json.loads(row[\"sentences\"])\n",
    "segments = json.loads(row[\"segments\"])\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Gold segments:\", segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f41d0",
   "metadata": {},
   "source": [
    "It can be seen that I have 9 sentences - first four in one topic, then two more in another and the last three in a different topic.\n",
    "## Sentence Embeddings\n",
    "In order to work with these sentences in any way, they should be converted to numbers/vectors. After doing research on that, using a transformers model turned out to be the best option since it captures semantic meaning which is important for our purpose. Here I will use SBERT model that provides sentence-level embeddings which will help for the detection of topics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (9, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(sentences, batch_size=8, show_progress_bar=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7081c",
   "metadata": {},
   "source": [
    "## Lexical Cohesion Method - Text Tiling\n",
    "This function implements a segmentation method inspired by TextTiling, but instead of relying on word frequencies, it uses sentence embeddings to detect topic shifts more efficiently. It slides a window across the text, compares the average embeddings of adjacent windows with cosine similarity, and marks a boundary whenever the similarity drops below a chosen threshold. The output is a list of sentence indices where topic boundaries are predicted, making it useful for splitting text into coherent sections based on semantic changes. I am going to use the base most pupular thresholds that are supposed to provide the best segments - 0.8 and window size - 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae85caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted boundaries at sentence indices: [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def embedding_text_tiling(embeddings, window_size=2, threshold=0.8):\n",
    "  \n",
    "    num_sentences = embeddings.shape[0]\n",
    "    boundaries = []\n",
    "    \n",
    "    for i in range(num_sentences - window_size):\n",
    "        block1 = embeddings[i:i+window_size].mean(axis=0)\n",
    "        block2 = embeddings[i+1:i+1+window_size].mean(axis=0)\n",
    "        sim = cosine_similarity(block1.reshape(1,-1), block2.reshape(1,-1))[0][0]\n",
    "        if sim < threshold:\n",
    "            boundaries.append(i+window_size-1)\n",
    "    return boundaries\n",
    "\n",
    "boundaries = embedding_text_tiling(embeddings, window_size=2, threshold=0.8)\n",
    "print(\"Predicted boundaries at sentence indices:\", boundaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb376db",
   "metadata": {},
   "source": [
    "We can see that it predicted a lot of boundaries which is not our case but still let's get the topic segments themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b202cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted segments: [0, 0, 1, 2, 3, 4, 5, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "def boundaries_to_segments(boundaries, num_sentences, min_size=1):\n",
    "    segments = [0] * num_sentences\n",
    "    current = 0\n",
    "    last_boundary = -1\n",
    "    filtered_boundaries = []\n",
    "\n",
    "    for b in boundaries:\n",
    "        if b - last_boundary >= min_size:\n",
    "            filtered_boundaries.append(b)\n",
    "            last_boundary = b\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        segments[i] = current\n",
    "        if i in filtered_boundaries:\n",
    "            current += 1\n",
    "    return segments\n",
    "\n",
    "\n",
    "pred_segments = boundaries_to_segments(boundaries, len(sentences))\n",
    "print(\"Predicted segments:\", pred_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281120a",
   "metadata": {},
   "source": [
    "We see again that this model oversegments the text and put almost every new sentence as new topic. \n",
    "## Sequential Thresholding\n",
    "This function segments text based on sentence embeddings. It works by comparing each sentence embedding to the one immediately before it using cosine similarity. If the similarity falls below the threshold, it assumes a topic shift has occurred and starts a new segment. Each sentence is then assigned a segment label, producing a list of segment indices that indicate how the text is divided into semantically coherent chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2803aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_thresholding(embeddings, threshold=0.75):\n",
    "    num_sentences = embeddings.shape[0]\n",
    "    pred_segments = [0]  \n",
    "    current_segment = 0\n",
    "\n",
    "    for i in range(1, num_sentences):\n",
    "        sim = cosine_similarity(\n",
    "            embeddings[i-1].reshape(1,-1), embeddings[i].reshape(1,-1)\n",
    "        )[0][0]\n",
    "        if sim < threshold:\n",
    "            current_segment += 1\n",
    "        pred_segments.append(current_segment)\n",
    "\n",
    "    return pred_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7111cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted segments: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "pred_segments = sequential_thresholding(embeddings, threshold=0.75)\n",
    "print(\"Predicted segments:\", pred_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c75ed",
   "metadata": {},
   "source": [
    "We have the same situation here as before - oversegmentation. The next method I am going to try is the very populat clustering.\n",
    "## Sequential Clustering\n",
    "This function applies adaptive sequential thresholding for topic segmentation, which is a refinement of the basic sequential method. Instead of comparing each sentence only to the previous one, it compares the current sentence embedding to the centroid (average vector) of the ongoing segment. If the similarity to the centroid drops below the threshold, it signals a topic change and starts a new segment; otherwise, the sentence is added to the current segment and the centroid is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8778063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted segments: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "def adaptive_sequential_thresholding(embeddings, threshold=0.8):\n",
    "    pred_segments = [0]\n",
    "    current_segment = 0\n",
    "    segment_vectors = [embeddings[0]]  \n",
    "\n",
    "    for i in range(1, len(embeddings)):\n",
    "        centroid = np.mean(segment_vectors, axis=0)\n",
    "        sim = cosine_similarity(embeddings[i].reshape(1, -1), centroid.reshape(1, -1))[0][0]\n",
    "        if sim < threshold:\n",
    "            current_segment += 1\n",
    "            segment_vectors = [embeddings[i]] \n",
    "        else:\n",
    "            segment_vectors.append(embeddings[i])\n",
    "        pred_segments.append(current_segment)\n",
    "\n",
    "    return pred_segments\n",
    "\n",
    "pred_segments = adaptive_sequential_thresholding(embeddings, threshold=0.8)\n",
    "print(\"Predicted segments:\", pred_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604527fe",
   "metadata": {},
   "source": [
    "Unfortunately we have the same situation here, so I will try another more complex approach - biclustering.\n",
    "## BATS\n",
    "This function implements a BATS-style segmentation approach, which clusters sentences into topics using word distributions rather than embeddings. It first builds a sentence–word TF-IDF matrix, then prunes out low-variance (uninformative) words and boosts highly discriminative ones to emphasize meaningful differences. With this refined matrix, it applies spectral biclustering, a method that simultaneously groups sentences and words into coherent clusters. Finally, it converts the resulting sentence cluster assignments into linear segments by marking boundaries whenever the cluster label changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058cf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bats_segmentation(sentences, n_topics=2, noise_thresh=0.001, boost_factor=2.0):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    M = vectorizer.fit_transform(sentences).toarray()  \n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    col_vars = np.var(M, axis=0)\n",
    "    keep_mask = col_vars > noise_thresh\n",
    "    M = M[:, keep_mask]\n",
    "    col_vars = col_vars[keep_mask]\n",
    "    words = [w for w, k in zip(words, keep_mask) if k]\n",
    "\n",
    "    if len(words) == 0:\n",
    "        raise ValueError(\"No informative words left after pruning. Try lowering noise_thresh.\")\n",
    "\n",
    "    var_norm = col_vars / (col_vars.max() + 1e-9)\n",
    "    boost = 1 + (boost_factor - 1) * var_norm\n",
    "    M = M * boost[np.newaxis, :]\n",
    "\n",
    "    model = SpectralCoclustering(n_clusters=n_topics, random_state=42)\n",
    "    model.fit(M)\n",
    "    sent_labels = model.row_labels_\n",
    "\n",
    "    boundaries = []\n",
    "    for i in range(1, len(sent_labels)):\n",
    "        if sent_labels[i] != sent_labels[i-1]:\n",
    "            boundaries.append(i-1)\n",
    "\n",
    "    segments = [0] * len(sentences)\n",
    "    current = 0\n",
    "    for i in range(len(sentences)):\n",
    "        segments[i] = current\n",
    "        if i in boundaries:\n",
    "            current += 1\n",
    "\n",
    "    return segments, boundaries, sent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0edf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted boundaries at indices: [0, 1, 3, 5, 6]\n",
      "Predicted segments: [0, 1, 2, 2, 3, 3, 4, 5, 5]\n",
      "\n",
      "--- Segment 0 ---\n",
      "Today I had a meeting with my semester coach and we started by discussing my individual project.\n",
      "\n",
      "--- Segment 1 ---\n",
      " I had some troubles in the beginning but now everything is clear.\n",
      "\n",
      "--- Segment 2 ---\n",
      " I enjoy the topic I chose and I am very happy with the progress I make.\n",
      " I look formard to finish it and see the end product.\n",
      "\n",
      "--- Segment 3 ---\n",
      " Then I talked with one of my team mates regarding our group work together because I was not satisfied with his way of working.\n",
      " He always misses deadlines and skips our group meetings and I suggested that he tries to put more effort.\n",
      "\n",
      "--- Segment 4 ---\n",
      "Finally, I got home and saw my mother making my favourite meal.\n",
      "\n",
      "--- Segment 5 ---\n",
      " I have always loved her cooking and appreaciate that she does it for me.\n",
      " Then we sat together on the table, ate the dinner and talked about our days. \n"
     ]
    }
   ],
   "source": [
    "pred_segments, boundaries, raw_labels = bats_segmentation(sentences, n_topics=2)\n",
    "\n",
    "print(\"Predicted boundaries at indices:\", boundaries)\n",
    "print(\"Predicted segments:\", pred_segments)\n",
    "\n",
    "for seg_id in set(pred_segments):\n",
    "    print(f\"\\n--- Segment {seg_id} ---\")\n",
    "    for s, seg in zip(sentences, pred_segments):\n",
    "        if seg == seg_id:\n",
    "            print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926eb6dd",
   "metadata": {},
   "source": [
    "We can see a little bit of improvement - some sentences are correctly put into one topic but still the segmentation is not accurate enough. \n",
    "\n",
    "The reason for all of these unsuccessful attempts is that BERT embeddings provide very sensitive vectors because they capture context. This make almost every new appearing word seem like a new topic - lower similarity. For this reason, I decided to implement addaptive threshold for the similarities which may help predict correct segments.\n",
    "\n",
    "## Adaptive threshold segmentation\n",
    "This function performs adaptive threshold segmentation by dynamically setting the similarity cutoff instead of using a fixed value. It first computes cosine similarities between consecutive sentence embeddings, then calculates a threshold either by subtracting a multiple of the standard deviation from the mean similarity (std method) or by selecting a percentile of the similarity distribution (percentile method). Using this adaptive threshold, the function detects boundaries whenever similarity drops low enough, while enforcing a minimum segment size to avoid over-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33e9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_threshold_segmentation(\n",
    "    embeddings, method=\"std\", min_size=2, std_factor=1.0, percentile=20\n",
    "):\n",
    "   \n",
    "    num_sentences = embeddings.shape[0]\n",
    "    sims = []\n",
    "\n",
    "    for i in range(1, num_sentences):\n",
    "        sim = cosine_similarity(\n",
    "            embeddings[i-1].reshape(1,-1), embeddings[i].reshape(1,-1)\n",
    "        )[0][0]\n",
    "        sims.append(sim)\n",
    "    \n",
    "    sims = np.array(sims)\n",
    "\n",
    "    if method == \"std\":\n",
    "        threshold = sims.mean() - std_factor * sims.std()\n",
    "    elif method == \"percentile\":\n",
    "        threshold = np.percentile(sims, percentile)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'std' or 'percentile'\")\n",
    "    \n",
    "    pred_segments = [0]\n",
    "    current_segment = 0\n",
    "    last_boundary = 0\n",
    "    \n",
    "    for i in range(1, num_sentences):\n",
    "        sim = cosine_similarity(\n",
    "            embeddings[i-1].reshape(1,-1), embeddings[i].reshape(1,-1)\n",
    "        )[0][0]\n",
    "        \n",
    "        if sim < threshold and (i - last_boundary) >= min_size:\n",
    "            current_segment += 1\n",
    "            last_boundary = i\n",
    "        pred_segments.append(current_segment)\n",
    "    \n",
    "    return pred_segments, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02373f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive threshold used: 0.28152403\n",
      "Predicted segments: [0, 0, 0, 0, 1, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "pred_segments, used_threshold = adaptive_threshold_segmentation(\n",
    "    embeddings, method=\"percentile\", percentile=30, min_size=2\n",
    ")\n",
    "print(\"Adaptive threshold used:\", used_threshold)\n",
    "print(\"Predicted segments:\", pred_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90bb77",
   "metadata": {},
   "source": [
    "Finally, we have the absolute correct segments.Adaptive threshold segmentation works especially well for short personal journals with BERT embeddings because it doesn’t rely on a rigid, one-size-fits-all cutoff for detecting topic shifts. In journals, the writing style is often fragmented, with sudden changes of mood, subject, or reflection, but also stretches where sentences remain semantically close. Fixed-threshold methods may either split too aggressively or miss subtle transitions. By calibrating the threshold based on the distribution of similarities in the specific text—using either standard deviation or percentiles—this method adapts to the natural “texture” of each journal entry. In practice, it means the algorithm is sensitive to real shifts in thought while filtering out small fluctuations that are just noise, making it better aligned with the irregular, personal style of diary-like writing.\n",
    "\n",
    "## Topic names\n",
    "This function identifies the most representative topic words for each text segment by applying TF-IDF (Term Frequency–Inverse Document Frequency). It first groups sentences according to their segment IDs, then merges all the sentences in a segment into one block of text. For each block, it calculates TF-IDF scores, which highlight words that are frequent in that segment but relatively uncommon overall. The top-scoring words are selected as the “topic words” for that segment. The function returns a dictionary mapping each segment ID to its top words, providing a concise summary of what each segment is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5217cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_topics(sentences, segments, top_n=1):\n",
    "    segment_dict = {}\n",
    "    unique_segments = sorted(set(segments))\n",
    "\n",
    "    for seg_id in unique_segments:\n",
    "        seg_sentences = [s for s, seg in zip(sentences, segments) if seg == seg_id]\n",
    "        seg_text = \" \".join(seg_sentences)\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        X = vectorizer.fit_transform([seg_text])\n",
    "        feature_array = np.array(vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = X.toarray()[0]\n",
    "\n",
    "        top_indices = tfidf_scores.argsort()[::-1][:top_n]\n",
    "        top_words = feature_array[top_indices].tolist()\n",
    "        segment_dict[seg_id] = top_words\n",
    "\n",
    "    return segment_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0 topic: troubles\n",
      "Segment 1 topic: group\n",
      "Segment 2 topic: talked\n"
     ]
    }
   ],
   "source": [
    "segment_topics = get_segment_topics(sentences, pred_segments, top_n=1)\n",
    "\n",
    "for seg_id, words in segment_topics.items():\n",
    "    print(f\"Segment {seg_id} topic: {words[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd116ad",
   "metadata": {},
   "source": [
    "This method doesn’t always provide the best topics because TF-IDF only looks at word frequency patterns, not the deeper semantic meaning of sentences. In short personal journals, important themes may be expressed with subtle wording, synonyms, or emotional nuance that TF-IDF can’t capture. It also tends to overemphasize rare but unimportant words (like a quirky adjective or a specific name) rather than core ideas. Since it ignores context and relations.\n",
    "## Topic names with BERT embeddings\n",
    "This function tries to find a meaningful topic word for each segment by combining BERT-style embeddings with linguistic filtering. Instead of relying on TF-IDF, it first computes a centroid embedding for all sentences in a segment (a kind of “semantic average” of that segment). Then, it extracts candidate nouns and proper nouns from the segment text using spaCy, since these are more likely to represent concrete topics. Each candidate word is embedded, and the one whose embedding is closest to the segment centroid is chosen as the representative topic. The result is a dictionary mapping segment IDs to their most semantically relevant noun(s), which usually produces more meaningful and context-aware topics than simple frequency-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def segment_topic_embedding(sentences, segments, sentence_embeddings, top_n=1):\n",
    "    segment_dict = {}\n",
    "    unique_segments = sorted(set(segments))\n",
    "\n",
    "    for seg_id in unique_segments:\n",
    "        indices = [i for i, seg in enumerate(segments) if seg == seg_id]\n",
    "        if not indices:\n",
    "            continue\n",
    "    \n",
    "        seg_emb = sentence_embeddings[indices].mean(axis=0, keepdims=True)\n",
    "        \n",
    "        seg_text = \" \".join([sentences[i] for i in indices])\n",
    "        doc = nlp(seg_text)\n",
    "        candidates = [token.lemma_ for token in doc if token.pos_ in {\"NOUN\", \"PROPN\"}]\n",
    "\n",
    "        if not candidates:\n",
    "            segment_dict[seg_id] = [\"[no noun found]\"]\n",
    "            continue\n",
    "\n",
    "        candidate_embeddings = model.encode(candidates, convert_to_numpy=True)\n",
    "    \n",
    "        sims = cosine_similarity(seg_emb, candidate_embeddings)[0]\n",
    "        top_indices = sims.argsort()[::-1][:top_n]\n",
    "        top_words = [candidates[i] for i in top_indices]\n",
    "\n",
    "        segment_dict[seg_id] = top_words\n",
    " \n",
    "    return segment_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b85f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0 topic: project,progress\n",
      "Segment 1 topic: group,group\n",
      "Segment 2 topic: dinner,meal\n"
     ]
    }
   ],
   "source": [
    "segment_topics = segment_topic_embedding(sentences, pred_segments, embeddings)\n",
    "\n",
    "for seg_id, words in segment_topics.items():\n",
    "    print(f\"Segment {seg_id} topic: {words[0]},{words[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95d397",
   "metadata": {},
   "source": [
    "The next thing I would like to try out is to get the topics for the whole text and see if they resonate with the segment topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def text_topic_embedding(sentences, sentence_embeddings, top_n=3):\n",
    "    \"\"\"\n",
    "    Extract top-N topic words for the whole text (ignoring segments).\n",
    "    \n",
    "    sentences: list of sentence strings\n",
    "    sentence_embeddings: numpy array of sentence embeddings\n",
    "    top_n: number of topic words to return\n",
    "    \"\"\"\n",
    "    \n",
    "    seg_emb = sentence_embeddings.mean(axis=0, keepdims=True)\n",
    "   \n",
    "    full_text = \" \".join(sentences)\n",
    "    doc = nlp(full_text)\n",
    "\n",
    "    candidates = [token.lemma_ for token in doc if token.pos_ in {\"NOUN\", \"PROPN\"}]\n",
    "\n",
    "    if not candidates:\n",
    "        return [\"[no noun found]\"] * top_n\n",
    "\n",
    "    candidate_embeddings = model.encode(candidates, convert_to_numpy=True)\n",
    "\n",
    "    sims = cosine_similarity(seg_emb, candidate_embeddings)[0]\n",
    "\n",
    "    top_indices = sims.argsort()[::-1][:top_n]\n",
    "    top_words = [candidates[i] for i in top_indices]\n",
    "\n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7db20e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics for the whole text: ['dinner', 'project', 'progress']\n"
     ]
    }
   ],
   "source": [
    "topics = text_topic_embedding(sentences, embeddings, top_n=3)\n",
    "print(\"Top topics for the whole text:\", topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac862f",
   "metadata": {},
   "source": [
    "As we can see, this works better for journals because it links what’s being talked about (nouns) with how the segment feels semantically as a whole (embeddings). I would still like to try out and provide a bit more meaningful topics by incorporationg phrases.\n",
    "## Topic as a phrase\n",
    "This function extends the noun-based topic extraction to instead find representative phrases for each segment. It works by first computing the centroid embedding of all sentences in a segment, capturing the segment’s overall meaning. Then, instead of single words, it extracts noun chunks with spaCy—multi-word phrases like “my best friend” or “a stressful day”—but filters out overly long ones (max 4 words) to keep them concise. Each candidate phrase is embedded using SBERT, and the phrase whose embedding is closest to the segment centroid is chosen as the representative topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d40e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def segment_topic_phrase(sentences, segments, sentence_embeddings, top_n=1):\n",
    " \n",
    "    segment_dict = {}\n",
    "    unique_segments = sorted(set(segments))\n",
    "\n",
    "    for seg_id in unique_segments:\n",
    "        indices = [i for i, seg in enumerate(segments) if seg == seg_id]\n",
    "        if not indices:\n",
    "            continue\n",
    "        \n",
    "        seg_emb = sentence_embeddings[indices].mean(axis=0, keepdims=True)\n",
    "        \n",
    "        seg_text = \" \".join([sentences[i] for i in indices])\n",
    "        doc = nlp(seg_text)\n",
    "        candidates = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) <= 4]\n",
    "\n",
    "        if not candidates:\n",
    "            segment_dict[seg_id] = [\"[no phrase found]\"]\n",
    "            continue\n",
    "        \n",
    "        candidate_embeddings = model.encode(candidates, convert_to_numpy=True)\n",
    "        \n",
    "        sims = cosine_similarity(seg_emb, candidate_embeddings)[0]\n",
    "        top_indices = sims.argsort()[::-1][:top_n]\n",
    "        top_phrases = [candidates[i] for i in top_indices]\n",
    "\n",
    "        segment_dict[seg_id] = top_phrases\n",
    "\n",
    "    return segment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f5628c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0 topic phrase: my individual project\n",
      "Segment 1 topic phrase: our group work\n",
      "Segment 2 topic phrase: her cooking\n"
     ]
    }
   ],
   "source": [
    "segment_phrases = segment_topic_phrase(sentences, pred_segments, embeddings)\n",
    "\n",
    "for seg_id, phrases in segment_phrases.items():\n",
    "    print(f\"Segment {seg_id} topic phrase: {phrases[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eec054",
   "metadata": {},
   "source": [
    "This method performs well because it combines the semantic power of embeddings with the expressiveness of noun phrases. Unlike single words, short phrases capture context and nuance—important in personal journals where meaning often lies in small details like “long phone call” or “quiet evening walk.” By selecting the phrase closest to the segment’s overall embedding, it ensures the chosen label truly reflects the central theme of that part of the text. This makes it especially useful for creating human-readable summaries of segments, giving you intuitive and meaningful “tags” for different parts of a journal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
