{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:37.638672Z",
     "start_time": "2025-11-19T11:42:11.949034Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from faker import Faker\n",
    "import subprocess\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "OUTPUT_DIR = \"llm\"\n",
    "NUM_SAMPLES = 50\n",
    "\n",
    "TOPICS = [\n",
    "    \"relationships\", \"school\", \"work\", \"daily life\",\n",
    "    \"health\", \"stress\", \"money\", \"habits\", \"goals\", \"emotions\"\n",
    "]\n",
    "\n",
    "def generate_segment_with_llm(topic):\n",
    "    prompt = f\"\"\"\n",
    "Write a short personal journal segment (3-10 sentences) about the topic \"{topic}\".\n",
    "Style: casual, slightly emotional, reflective, natural phrasing.\n",
    "Fully synthetic. Do NOT use any real names or real places.\n",
    "\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"llama3.2:3b\"],\n",
    "        input=prompt,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    return result.stdout.strip()\n",
    "\n",
    "def inject_realistic_pii(sentences):\n",
    "    pii_items = []\n",
    "\n",
    "    synthetic_pii_patterns = [\n",
    "        (\"PERSON\", fake.name),\n",
    "        (\"EMAIL\", fake.email),\n",
    "        (\"PHONE\", fake.phone_number),\n",
    "        (\"ADDRESS\", lambda: fake.address().replace(\"\\n\", \", \"))\n",
    "    ]\n",
    "\n",
    "    num_pii = random.randint(1, 4)\n",
    "    chosen_sentences = random.sample(range(len(sentences)), num_pii)\n",
    "\n",
    "    for idx in chosen_sentences:\n",
    "        pii_type, generator = random.choice(synthetic_pii_patterns)\n",
    "        value = generator()\n",
    "\n",
    "        sentences[idx] += f\" You can reach them at {value}.\"\n",
    "\n",
    "        pii_items.append({\n",
    "            \"type\": pii_type,\n",
    "            \"text\": value,\n",
    "            \"sentence_index\": idx\n",
    "        })\n",
    "\n",
    "    return sentences, pii_items\n",
    "\n",
    "def generate_sample(i):\n",
    "\n",
    "    num_segments = random.randint(2, 5)\n",
    "    topics = random.sample(TOPICS, num_segments)\n",
    "\n",
    "    segment_texts = []\n",
    "\n",
    "    # STEP 1 â€” generate segments via LLM\n",
    "    for topic in topics:\n",
    "        seg = generate_segment_with_llm(topic)\n",
    "        segment_texts.append(seg)\n",
    "\n",
    "    # STEP 2 â€” merge + compute offsets later\n",
    "    sentences = []\n",
    "    segment_boundaries = []\n",
    "    cursor = 0\n",
    "\n",
    "    for seg_id, (topic, seg_text) in enumerate(zip(topics, segment_texts)):\n",
    "        seg_sentences = [s.strip() for s in seg_text.split(\".\") if s.strip()]\n",
    "        segment_sentence_count = len(seg_sentences)\n",
    "\n",
    "        start_char = None\n",
    "        end_char = None\n",
    "\n",
    "        for s in seg_sentences:\n",
    "            if start_char is None:\n",
    "                start_char = cursor\n",
    "            sentences.append(s)\n",
    "            cursor += len(s) + 2\n",
    "\n",
    "        end_char = cursor\n",
    "\n",
    "        segment_boundaries.append({\n",
    "            \"id\": seg_id,\n",
    "            \"topic\": topic,\n",
    "            \"start_char\": start_char,\n",
    "            \"end_char\": end_char,\n",
    "            \"text\": seg_text\n",
    "        })\n",
    "\n",
    "    sentences, pii_raw = inject_realistic_pii(sentences)\n",
    "\n",
    "    transcript = \"\"\n",
    "    offsets = []\n",
    "\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if idx > 0:\n",
    "            transcript += \" \"\n",
    "        base_pos = len(transcript)\n",
    "        transcript += sentence + \".\"\n",
    "\n",
    "        for item in pii_raw:\n",
    "            if item[\"sentence_index\"] == idx:\n",
    "                value = item[\"text\"]\n",
    "                start = transcript.index(value)\n",
    "                end = start + len(value)\n",
    "                offsets.append({\n",
    "                    \"type\": item[\"type\"],\n",
    "                    \"text\": value,\n",
    "                    \"start_char\": start,\n",
    "                    \"end_char\": end\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        \"transcript_id\": f\"synthetic_{i:04d}\",\n",
    "        \"transcript\": transcript,\n",
    "        \"segments\": segment_boundaries,\n",
    "        \"topics\": topics,\n",
    "        \"pii\": offsets,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    for i in range(1, NUM_SAMPLES + 1):\n",
    "        try:\n",
    "            sample = generate_sample(i)\n",
    "            with open(f\"{OUTPUT_DIR}/{i:04d}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(sample, f, indent=2)\n",
    "            print(f\"Generated {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on sample {i}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1\n",
      "Generated 2\n",
      "Generated 3\n",
      "Generated 4\n",
      "Generated 5\n",
      "Generated 6\n",
      "Generated 7\n",
      "Generated 8\n",
      "Generated 9\n",
      "Generated 10\n",
      "Generated 11\n",
      "Generated 12\n",
      "Generated 13\n",
      "Generated 14\n",
      "Generated 15\n",
      "Generated 16\n",
      "Generated 17\n",
      "Generated 18\n",
      "Generated 19\n",
      "Generated 20\n",
      "Generated 21\n",
      "Generated 22\n",
      "Generated 23\n",
      "Generated 24\n",
      "Generated 25\n",
      "Generated 26\n",
      "Generated 27\n",
      "Generated 28\n",
      "Generated 29\n",
      "Generated 30\n",
      "Generated 31\n",
      "Generated 32\n",
      "Generated 33\n",
      "Generated 34\n",
      "Generated 35\n",
      "Generated 36\n",
      "Generated 37\n",
      "Generated 38\n",
      "Generated 39\n",
      "Generated 40\n",
      "Generated 41\n",
      "Generated 42\n",
      "Generated 43\n",
      "Generated 44\n",
      "Generated 45\n",
      "Generated 46\n",
      "Generated 47\n",
      "Generated 48\n",
      "Generated 49\n",
      "Generated 50\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
